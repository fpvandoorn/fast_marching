
\documentclass[12pt]{article}
\usepackage[mathletters]{ucs}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{lmodern}

\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{graphicx}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{tabularx}
\usepackage{fancyhdr}
\usepackage[french]{babel}
% \usepackage[style=numeric]{biblatex}
% \addbibresource{bib 3.bib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumerate}

\usepackage[notref]{showkeys} % Commenter avant d'imprimer ou d'envoyer le rapport

\usepackage{bm}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{braket}
\usepackage{tikz}
\usepackage{tikz-feynman}
\usepackage{caption}
\usepackage{physics}
\usetikzlibrary{patterns}
\usepackage{todonotes}
\def\bZ{\mathbb{Z}}
\def\cD{\mathcal{D}}
\def\cM{\mathcal{M}}
\def\cO{\mathcal{O}}
\def\bR{\mathbb{R}}
\DeclareMathOperator\len{len}
\DeclareMathOperator\Id{Id}
\newcommand\todojm[1]{\todo[inline]{JMM : #1}}

\newtheorem{theorem}{Th\'eor\`eme}[section]
\newtheorem{remark}[theorem]{Remarque}
\newtheorem{lemma}[theorem]{Lemme}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollaire}
\newtheorem{definition}[theorem]{D\'efinition}

\usepackage[hidelinks]{hyperref}

%pour ins√©rer code √©ventuellement
\usepackage{xcolor}
\usepackage{mathtools,upgreek}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1}   % red
\definecolor{tacticcolor}{rgb}{0.0, 0.1, 0.3}    % dark blue
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{stringcolor}{rgb}{0.5, 0.3, 0.2}    % brown
\definecolor{symbolcolor}{rgb}{0.1, 0.2, 0.7}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green
\definecolor{attributecolor}{rgb}{0.7, 0.1, 0.1} % red
\definecolor{errorcolor}{rgb}{1, 0, 0}           % bright red

\usepackage{listings}
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean,breakatwhitespace}
% fin code

\renewcommand*\contentsname{\textsc{Table des mati√®res}}
\def\nbR{\ensuremath{\mathrm{I\! R}}}
\usepackage{subcaption}
\renewcommand\thesection{\Roman{section}}
\pagestyle{fancy}
\usepackage{graphicx}
\fancyhf{}
\rhead{2023}
\lhead{\includegraphics[width=2cm]{ENScr.png}} % 1,7 avant pour width
\chead{Rapport de stage L3}
\cfoot{\thepage}
\usepackage[nottoc]{tocbibind}
\numberwithin{equation}{section}


\begin{document}
\topskip0pt
\begin{center}
    \includegraphics[width=0.5\textwidth]{ENSPS_UPSAY_logo_couleur_2.png}
\end{center}

\begin{center}
    \vspace{30pt}
    \Large\textsc{Ecole Normale Sup√©rieure Paris-Saclay}
    \\
    \large\textsc{D√©partement de Math√©matiques}
    \\
    \textsc{}
    \\
    \rule{200pt}{0.5pt}
    \\
    \vspace{10pt}
    {Stage de recherche - licence 3 math√©matiques}
    \\
    \vspace{50pt}
    \LARGE\textsc{Formalisation de l'algorithme du Fast-Marching ; D√©composition de Selling et consistance du sch√©ma}
    \\
    \vspace{40pt}
    \large{Rapport de stage de}

    \Large\textsc{Matteo Guicheneuy et Ang√®le Lochet}
    \\
    \large{supervis√©s par}

    \Large\textsc{Jean-Marie Mirabeau et Floris Van Doorn}

    \vspace{40pt}
    \rule{200pt}{0.5pt}

    \vspace{10pt}
    \large{28 Mars 2023 - 1 Juillet 2023}
\end{center}
\thispagestyle{empty}
\newpage

\tableofcontents
\setcounter{page}{1}



\newpage
\section*{R√©sum√©}

\section{Probl√©matique}

\section{Introduction}
\subsection{L'√©quation eikonale}
L'√©quation eikonale nous vient de la physique, et plus pr√©cis√©ment de l'optique, c'est l'√©quation qui permet de d√©crire la propagation de la lumi√®re dans un milieu.
Notre int√©r√™t se porte sur cette √©quation en raison du fait qu'elle puisse √™tre utilis√© pour r√©soudre des probl√®mes de trajectoire optimale.

\todojm{Mettre quelques r√©f√©rences (optique, trajectoire optimale)}

Commen√ßons par introduire quelques notions importantes:

On se place dans $\Omega$ un ouvert born√© de $\bR^d$.
Soit $\cM$ une m√©trique Riemannienne sur le domaine  $\Omega$:
\begin{equation*}
    \cM \in C^0(\bar{\Omega},S_d^{++})
\end{equation*}
Une m√©trique est un outil qui nous permet de mesurer des longueurs dans notre espace, On peut la voir comme une fonction qui √† un point de $\bar{\Omega}$ associe une distance ($S_d^{++}$).

\todojm{La fin de phrase ``une distance ($S_d^{++}$)''  n'est pas claire. Proposition : associe au point $x$ la norme sur l'espace tangent d√©finie par la matrice $\cM(x)\in S_d^{++}$}

A partir de la m√©trique on peut definir la longueur d'un chemin $\gamma: [0,1]\to \bar{\Omega}$, lipschitz:
\begin{equation*}
    \len_{\cM}(\gamma)=\int_0^1 \| \gamma'(t) \|_{\cM (\gamma(t))}dt.
\end{equation*}
Ainsi, on d√©finit $u : \overline \Omega\to \bR$ la distance au bord de $\Omega$:
\begin{equation*}
    u(x) = \inf\{ \len_\cM(\gamma)\mid \gamma(0)=x, \gamma(1)\in \partial \Omega\}.
\end{equation*}

\todojm{Distinguer les √©galit√©s par d√©finition ``$:=$'' de celles qui sont des propri√©t√©s ``$=$''}

Il est possible de montrer que r√©soudre l'√©quation eikonale Riemannienne suivante:
\begin{equation*}
\begin{cases}
    \| \nabla u(x) \|_{\cD(x)}=1, & x\in \Omega, \\
    u(x)=0, & x \in \partial \Omega.
\end{cases}
\end{equation*}
permet de trouver le chemin le plus court, qui atteint l'infimum dans la d√©finition de $u(x)$,
avec $\cD:=\cM^{-1}$
\subsection{L'algorithme du fast-marching}

Introduction du sch√©ma isotrope dans Rouy \cite{rouy1992viscosity}.

Preuve qu'il se r√©sout en une passe sur le domaine dans \cite{tsitsiklis1995efficient,sethian1996fast}.

Parler de qq g√©n√©ralisations ult√©rieures.

\todo[inline]{Expliquer que, compte tenu de la difficult√© de la formalisation, l'on s'est finalement focalis√© sur deux lemmes importants : la d√©composition de Selling et la consistance du sch√©ma.}


\subsection{L'assistant de preuve LEAN}

Donner quelques motivations pour la formalisation \cite{massot2021formalize}.

Exemples de projets.


\section{D√©composition de Selling}

\subsection{Pr√©sentation usuelle}

\begin{definition}[Superbase]
\label{def:Superbase}
On dit que $(v_0,...,v_d) \in (\bZ^d)^{d+1} $ est une superbase, si cette famille de vecteurs v√©rifie :
\begin{equation}
    \sum_{0\leq i\leq d} v_i = 0 \text{ et } \lvert  \det (v_1,...,v_d)  \lvert  =  1
\end{equation}
\end{definition}

Ex : Voir la D√©finition \ref{def:Superbase}

Par la suite, fixons-nous un $x \in \Omega $ et notons $D \equiv \cD (x) $ avec $\cD (x)$ d√©finie pr√©c√©demment.
Avec la d√©finition \ref{def:Superbase}, nous pouvons montrer la formule de Selling, n√©cessaire pour l'algorithme du fast-marching :

\todojm{Plus pr√©cis√©ment, cette formule est n√©cessaire pour la version Riemannienne du fast marching. Citer la ref.}


\begin{lemma}[Formule de Selling]
\label{lem:FormuleSelling}
 Soit $D \in S_d $. Soit $(v_0,...,v_d)$ une superbase. On a alors :
\begin{equation}
\label{AssociatedVectors}
        D = - \sum_{0\leq i < j \leq d} \braket{ v_i }{ D v_j } e_{ij} e_{ij}^T \text{ avec } \braket{e_{ij}}{ v_k } = \delta_{ik} - \delta_{kj}
\end{equation}
Les vecteurs $e_{ij}$ se nomment les vecteurs associ√©s.
\end{lemma}

\todo[inline]{
Ca pourrait valoir la peine d'introduire les vecteurs associ√©s dans un Lemme au dessus. (Et justifier qu'ils sont bien d√©finis.)
}


\begin{proof}
Pour plus de clart√©, notons $D' =  - \sum_{0\leq i < j \leq d} \braket{ v_i}{ D  v_j } e_{ij} e_{ij}^T$. Calculons alors, pour tout couple $(k,l)$ tel que $0 \leq k < l \leq d $ :
\begin{equation}
\label{form_sell}
    \begin{split}
        \braket{v_k}{D' v_l} & = - \sum_{0\leq i < j \leq d}\braket{ v_i}{ D  v_j } v_{k}^T  e_{ij} e_{ij}^T v_l\\
        & = - \sum_{0\leq i < j \leq d}\braket{ v_i}{ D  v_j }  \braket{v_{k}}{e_{ij}} \braket{e_{ij}}{v_l}\\
        & = + \braket{ v_k }{ D  v_l } \\
    \end{split}
\end{equation}
Pour passer de la deuxi√®me √† la derni√®re ligne, nous utilisons la d√©finition des vecteurs $e_{ij}$ donn√©e par \ref{AssociatedVectors}, en simplifiant sachant que $k<l$ et que $i<j$. On obtient la m√™me relation pour $k>l$ en suivant le m√™me m√©chanisme (A VERIFIER).
\\
De plus, pour tout $i$, $\braket{v_i}{D' v_i} = \braket{v_i}{D v_i}$ : en effet il suffit de remplacer $v_i$ par $(-v_0-v_1-...-v_{i-1}-v_{i+1}-...-v_{d})$ (d'apr√®s la d√©finition d'une superbase) et d'utiliser \ref{form_sell}.
\end{proof}

\todojm{Conclure : les formes quadratiques d√©finies par $D$ et $D'$ ont m√™me matrice dans la base $(v_1,\cdots,v_d)$.}


\begin{definition}[famille D-obtuse]
\label{Dobtuse}
    On dit que $(v_0,...,v_d) \in (\bZ^d)^{d+1} $ est D-obtue avec $ D \in S_d^{++}$, si :
    \begin{equation}
    \braket{v_i}{D v_j} < 0  \forall  0\leq i < j \leq d
\end{equation}
\end{definition}

\todojm{obtue -> obtuse}

\todojm{Ne pas h√©siter √† commenter les points difficiles √† formaliser des preuves. Eventuellement proposer des preuves alternatives qui se formalisent mieux.}

{Lorsque $ D \in S_d^{++}$ (ce qui est le cas ici) et lorsque la superbase est D-obtuse, alors la formule de Selling est appel√©e d√©composition de Selling. }

\todojm{Eviter les accolades inutiles autour des paragraphes dans le code source. Cr√©e des risques d'erreur.}

{C'est cette d√©composition qui est n√©cessaire dans l'algorithme du fast-marching. Il ne nous reste donc plus qu'√† prouver l'existence d'une superbase D-obtuse, afin d'obtenir la d√©composition de Selling. Cette d√©monstration est faite ici pour $d=2,3$ de mani√®re constructive gr√¢ce √† l'algorithme de Selling.}
\paragraph{Sch√©ma : algorithme de Selling pour $d = 2$:}

{\let\clearpage\relax\include{algorithme de Selling}}

\todojm{Utiliser le package enumerate, ou itemize.}

\begin{enumerate}[I]
    \item Premier point
    \begin{enumerate}[{I.}1]
        \item Premier sous-point
        \item Deuxi√®me sous-point
    \end{enumerate}
    \item Deuxi√®me point
\end{enumerate}


{Il ne nous reste plus qu'√† prouver que cet algorithme termine et qu'il renvoie bien une superbase D-obtuse.}
\begin{proof}[D√©monstration : Algorithme de Selling]
La d√©monstration s'effectue en deux temps :
\\
I. V√©rification que si l'algorithme termine, la base retourn√©e est bien une superbase D - obtuse:
\\

\todojm{``La base retourn√©e''. Une base de $Z^d$ a $d$ √©l√©ments, et non $d+1$. Dire plut√¥t la famille de vecteurs retourn√©e ou le $(d+1)$-uplet retourn√©.}

I.1. La base retourn√©e par l'algorithme est bien une superbase : en effet, la famille
\begin{equation}
    (- v_0, v_1, v_0 - v_1)
\end{equation}
est bien une superbase si $(v_0,v_1,v_2)$ l'est car la somme des vecteurs est bien nulle et une simple manipulation du d√©terminant en utilisant que la somme des vecteurs est nulle montre que ces deux familles ont en valeur absolue le m√™me d√©terminant. Cette transformation s'appelle la transformation de Selling. Ainsi, √† chaque √©tape de la boucle while, nous conservons une superbase.
\\
\todojm{``L'algorithme termine'' essayer autant que possible d'avoir une exposition lin√©aire des r√©sultats. Si ce n'est pas possible, bien faire r√©f√©rence au ``r√©sultat ci-dessous''}
I.2. L'algorithme termine, on obtient donc la n√©gation de la condition while :
\begin{equation}
    \forall i, j, \braket{v_i}{D v_j} \leq 0
\end{equation}
D'o√π par d√©finition d'une superbase D - obtuse, nous obtenons bien une superbase D -obtuse.
\\

II. D√©monstration de la terminaison de l'algorithme :
\\
Il nous faut d'abord d√©finir la notion de l'√©nergie d'une superbase :
\begin{definition}[Energie $E_D$ d'une superbase $b$]
\label{def:EnergieSuperbase}
    \begin{equation}
    E_D (b) = \sum_{0\leq i \leq d} \braket{v_i}{D v_i}
\end{equation}
\end{definition}

\\

√† chaque √©tape, l'√©nergie de la superbase cr√©√©e est inf√©rieure strictement √† celle pr√©c√©dente + nombre fini de superbase v√©rifiant cela --> terminaison de l'algorithme
\end{proof}


\subsection{Formalisation}

Il nous faut d'abord d√©finir les notions de superbases (D-obtuses) sur Lean. Une subtilit√© cependant : nous choisissons, sur Lean, de ne prendre en compte que les superbases directes (ie $\det (v_1,...,v_d) = 1 $). En effet, le temps de calcul sous Lean est √† prendre en compte, et la s√©paration des cas du d√©terminant valant $+1$ ou $-1$ compliquait et prolongeait √©norm√©ment les preuves. Afin d'obtenir les preuves pour le cas du d√©terminant n√©gatif, il suffit d'appliquer les m√™mes codes, avec la superbase indirecte (√† quelques subtilit√©s pr√®s).

\begin{definition}[famille D - obtuse sous Lean]
\label{DObtuseLean}

\end{definition}


\begin{definition}[Superbase directe sous Lean]
\label{SuperbaseLean}
    inserer code
\end{definition}

La premi√®re √©tape de ce stage consiste √† montrer la formule de Selling. Un lemme pr√©liminaire grandement n√©cessaire sous Lean, qui peut para√Ætre √©vident √† la main, est le suivant :
\begin{lemma}
\label{ExercisePartOne}
    inserer code v0 = -v1 - v2
\end{lemma}

En voici une autre version similaire tout aussi utile :
\begin{lemma}
\label{ExercisePartOneVar}
    inserer code v0 = -v1 - v2
\end{lemma}

\todojm{Discuter un peu de pourquoi ce n'est pas automatis√©. (Somme finie sur un ensemble donn√©.  Les implifications comme linarith s'appliquent √† des √©l√©ments d'anneaux plut√¥t que des vecteurs.)}

Il nous faut maintenant d√©finir les vecteurs associ√©s. Le fait de les d√©finir par rapport √† un produit scalaire para√Æt difficile sur Lean, qui a besoin d'une expression explicite. Or pour $d=2$, il est possible d'obtenir une telle expression de ces vecteurs :
\begin{lemma}
\label{VectAssoc}
    Soient $0\leq i < j \leq 2$. Soit $0 \leq k \leq 2$ tel que $k\ne i $ et $k \ne j$. Alors :
    \begin{equation}
        e_{ij}= \pm v_k^{\perp}
    \end{equation}
    Le signe est directement le signe de la permutation : $
      \begin{pmatrix}
    0 & 1 & 2 \\
    i & j & k
  \end{pmatrix} $
\end{lemma}
\begin{proof}
    D'apr√®s la d√©finition \ref{AssociatedVectors}, si $k\ne i $ et $k \ne j$ alors $\braket{e_{ij}}{ v_k } = 0$, soit $e_{ij} \in v_k^{\perp}$. Or nous sommes en dimension 3 donc $v_k^{\perp}$ correspond √† la droite dirig√©e par :
    \begin{equation}
        v_k^{\perp} = \begin{pmatrix}
            - (v_k)_1 \\
              (v_k)_0
        \end{pmatrix}
    \end{equation}
    Sous r√©serve d'orthonormalisation, on a donc bien : $e_{ij}= \pm v_k^{\perp}$. Pour le signe, il suffit de se rappeler que d'apr√®s \ref{AssociatedVectors} on doit aussi v√©rifier  $\braket{e_{ij}}{ v_i } = 1$ et $\braket{e_{ij}}{ v_j } = - 1$.



\end{proof}
\begin{lstlisting}
// def B.1
def is_superbase (v : matrix (fin (d+1)) (fin d) R) : Prop :=
    \sum i, v i = 0 ‚àß |(v.submatrix fin.succ id).det| = 1
\end{lstlisting}

Id√©e : faut-il remettre les d√©finitions √©crites en latex ? N'est-ce pas finalement bizarre de s√©parer, au moins pour les d√©finitions faudrait-il les rappeler ?
is direct superbase : choix de se r√©duire √† des superbases dont le d√©terminant vaut 1, car ne change rien √† la m√©thode mais change tout (nb de cas et tt) sur lean --> temps de calcul largement r√©duit (expliquer tactiques qui mettent du temps, car IA etc)
D√©crire les choix, les difficult√©s.

Code : mettre les √©nonc√©s, et d√©finitions, qq preuves si tr√®s courtes.
(Le reste en annexe.)



\section{Consistance du sch√©ma}

\subsection{Cas √† 1 dimension}


L'objectif de cette partie est de d√©montrer la consistance du sch√©ma 1D ($\Omega\subset \bR$):
\begin{lemma}
Soit $u: \Omega \to \bR$ $C^1$, alors pour tout $x\in\Omega$.
\begin{equation}
    max(0,u(x)-u(x-h),u(x)-u(x+h))=|hu'(x)|+o(h)
\end{equation}
\end{lemma}


\begin{proof}[D√©monstration usuel]
Commen√ßons dans un premier temps par la d√©monstration usuel du sch√©ma.

Soit $x\in \Omega$.

$u$ est une fonction diff√©rentiable sur $\Omega$, on a alors:
\begin{equation*}
\begin{split}
    u(x)-u(x+h)-hu'(x)=o(h) \\
    u(x)-u(x-h)+hu'(x)=o(h)
\end{split}
\end{equation*}
Par 1-lispchitzianit√© du max(voir annexe) appliqu√© au vecteur $(0,u(x)-u(x+h),u(x)-u(x-h))$ et $(0,hu'(x),-hu'(x))$ on obtient l'in√©galit√© suivante:
\begin{equation*}
\begin{split}
    |max(0,u(x)-u(x+h),u(x)-u(x-h))-max(0,hu'(x),-hu'(x)) | \\ \le max(|0-0|,|u(x)-u(x+h)-hu'(x)|,|u(x)-u(x-h)+hu'(x)|)
\end{split}
\end{equation*}
Soit apr√®s quelques simplifications:
\begin{equation*}
\begin{split}
    |\max(0,u(x)-u(x+h),u(x)-u(x-h))-|hu'(x)||\\
    \le \max(|u(x)-u(x+h)-hu'(x)|,|u(x)-u(x-h)+hu'(x)|)
\end{split}
\end{equation*}
le max de deux petits $o$ reste un petit $o$, on en d√©duit le r√©sultats escompt√©.
\end{proof}

Int√©ressons nous maintenant √† la d√©monstration de la consistance sous Lean.
\begin{proof}[D√©monstration sous Lean]

Dans un premier temps nous allons d√©montrer la 1-lipschitzianit√© du max pour des vecteur a 3 composantes.
Cette d√©monstration se fait sans trop de difficult√© √† partir de la 1-lipschitzianit√© du max d√©j√† pr√©sent dans Lean et de quelque in√©galit√© sur le max (voir annexe).
Avant de rentrer dans le coeur de la d√©monstration sous Lean int√©ressons nous √† la d√©finition des petits taux sous Lean.
\begin{lstlisting}[gobble=2]
    def is_o (l : filter Œ±) (f : Œ± ‚Üí E) (g : Œ± ‚Üí F) :
    Prop := ‚àÄ ‚¶Éc : ‚Ñù‚¶Ñ, 0 < c ‚Üí is_O_with c l f g
\end{lstlisting}
Dans cette d√©finition $f$ et $g$ sont des fonctions et $l$ est un ensemble contenant ... et $c$ est la constante de domination; le o est alors d√©finit par un O d√©finit ainsi:
\begin{lstlisting}[gobble=2]
    def is_O_with (c : ‚Ñù) (l : filter Œ±) (f : Œ± ‚Üí E) (g : Œ± ‚Üí F) : Prop :=
    ‚àÄ·∂† x in l, ‚Äñ f x ‚Äñ ‚â§ c * ‚Äñ g x ‚Äñ
\end{lstlisting}

\begin{remark}[Filtres et composition des limites]

\end{remark}

Une premi√®re √©tape de la d√©monstration est de d√©velopper au maximum les hypoth√®ses et l'objectif que l'on doit d√©montrer.
On construit ensuite l'ensemble $W$ qui est le nouveau filtre. On a alors trois propositions √† d√©montrer, les deux derniers sont simple √† d√©monter, elles consistent √† d√©montrer que $W$ est ouvert et qu'il contient $0$.
Le premier point est le plus technique, c'est dans celui ci que l'on montre le coeur de la preuve. C'est dans cette partie que l'on utilise la lipschitzianit√© du max. Par un jeu d'in√©galit√© et de re√©criture on arrive a nos fins.
Comme on peut le voir la preuve est long est tr√®s technique sous Lean, malgr√®s une preuve initiale simple.
\begin{lstlisting}[gobble=2]
    lemma max_0_u (u : ‚Ñù ‚Üí ‚Ñù) (x u': ‚Ñù) (hu : has_deriv_at u u' x) :
    (Œª h,  max 0 (max ((u x - u (x - h)) ) ((u x - u (x + h) ))) - |h  *u'|)=o[ùìù 0] Œª  h, h :=
    begin
        -- Reecriture des d√©finitions.
        rw is_o_iff,-- reecriture de la definition de o dans l'objectif:
        intros c hc,-- On fixe une
        rw eventually_nhds_iff,
        let h1 := (my_lemma u u' x).1 hu,
        rw is_o_iff at h1,
        specialize h1 hc,
        rw eventually_nhds_iff at h1,
        rcases h1 with ‚ü®V,  ‚ü®H, V_open, V0‚ü©‚ü©,

        -- Definition de W
        let W:= V ‚à© -V,
        use W,
        split,

        -- Coeur de la preuve:
        {intros h Wh,
        rw abs_eq_max_neg,
        repeat{rw real.norm_eq_abs},
        let max_diff := max_1_lip (u x - u (x - h))  (u x - u (x + h)) (h*u') (-(h  * u')) ,
        rw ‚Üê abs_eq_max_neg at max_diff,
        let P:= abs_nonneg (h * u'),
        rw max_eq_right P at max_diff,
        rw abs_eq_max_neg at max_diff,
        rw abs_eq_max_neg at max_diff,
        rw ‚Üê abs_eq_max_neg at max_diff,
        let diffp := H h Wh.1,
        repeat{rw real.norm_eq_abs at diffp},
        let diffm := H (-h) Wh.2,
        repeat{rw real.norm_eq_abs at diffm},
        rw abs_neg at diffm,
        rw ‚Üê abs_neg at diffm,
        rw ‚Üê abs_neg at diffp,
        let F := max_le diffp diffm,
        rw max_comm at F,
        apply le_trans max_diff _,
        apply le_trans _ F,
        simp only [‚Üê sub_eq_add_neg],
        apply le_of_eq,
        congr' 2; ring },
        split,

        -- Preuve que W est ouvert
        {have V_neg_open := V_open.neg,
         apply is_open.inter V_open V_neg_open,},

         --Preuve que $0\in W$
        {simp,
        exact V0,},
    end
\end{lstlisting}
\todo[inline]{Mettre des bouts de code illustrant les concepts Lean pour : o, max 1-Lip. Eventuellement parler du pb avec les valeurs dans $[0,\infty]$.}
\end{proof}




\subsection{Cas √† 2 dimension}

\todo[inline]{Utiliser l'environnement begin/end lemma, etc}

Dans cette partie nous traiterons le cas √† deux dimensions ($\Omega\subset \bR^2$).

Par soucie de simplicit√© d'√©criture d√©finissons $\delta^e_hu$:
\begin{definition}
Soit $x\in \Omega$ et $e\in \bR^2$
\begin{equation}
    \delta^e_hu(x)=max(0,u(x)-u(x-he),u(x)-u(x+he))
\end{equation}
\end{definition}


\todo[inline]{La consistance s'applique √† toute fonction $u \in C^1$. (La distance au bord n'est pas $C^1$.)}

\todo[inline]{Mettre une ref √† la th√©orie des solutions de viscosit√©. Il suffit de v√©rifier la consistance sur les fonctions lisses, m√™me si la fonction distance n'est pas lisse.
Ref classique par exemple : Crandall et Lions, User's guide to viscosity solutions.
}

La consitance du sch√©ma 2D est donn√© par le lemme suivant:
\begin{lemma}
\label{lemme:consistance_2D}
Soit $u: \Omega \to \bR$ une fonction $C^1$, alors pour tout $x\in\Omega$.
\begin{equation}
     \sum_{e \in E}\lambda(e)(\delta_e^hu(x))^2 = h^2<\nabla u(x), D\nabla u(x)>+o(h^2)
\end{equation}

\end{lemma}
Pour d√©montrer ce lemme nous allons avoir besoin d'un r√©sultat interm√©diaire:
\begin{lemma}
\label{lemme_interm√©dire}
Soient $u: \Omega \to \bR$ diff√©rentiable sur tout $\Omega$ et $e\in \bR^2$, alors pour tout $x\in\Omega$:
\begin{equation}
    \delta^e_hu(x)=|h<\nabla u(x),e>|+o(h)
\end{equation}
\end{lemma}

\begin{proof}[D√©monstrastion usuelle du Lemme \ref{lemme_interm√©dire}]
Commen√ßons par la d√©monstration usuel du lemme IV.4.

Soient $x\in \Omega$ et $e\in \bR^2$.

Posons:
\begin{equation*}
\begin{array}{ccccc}
    v &: & \bR & \to & \bR \\
     & & t & \mapsto &  u(x+te)
\end{array}
\end{equation*}

Par diff√©rentiabilit√© de $u$ en $x$, $v$ est d√©rivable en 0.

Par un calcul √©l√©mentaire de calcul diff√©rentiel on a: $v'(0)=<\nabla u(x),e>$.

En appliquant le lemme 4.1 √† $v$, on obtient le r√©sultat voulue.
\end{proof}
\begin{proof}[D√©monstrastion sous Lean du Lemme \ref{lemme_interm√©dire}]
  \label{D√©monstration sous Lean du lemme 4.3:}
  D√©montrons maintenant ce m√™me lemme sous lean.

\end{proof}

\begin{proof}[D√©monstration usuelle du lemme\ref{lemme:consistance_2D}]
Nous pouvons d√®s √† pr√©sent nous atteler √† la d√©monstration du lemme \ref{lemme:consistance_2D}:
\begin{equation}
    \begin{split}
        \sum_{e \in E}\lambda(e)(\delta_e^hu(x))^2 & =\sum_{e \in E}\lambda(e)(|h<\nabla u(x),e>|+o(h))^2\\
        & = h^2\sum_{e \in E}\lambda(e)|<\nabla u(x),e>|^2+o(h^2)\\
        & = h^2\sum_{e \in E}\lambda(e)|Tr(\nabla u(x)\nabla u(x)^T ee^T)+o(h^2)\\
        &  = h^2Tr(\nabla u(x)\nabla u(x)^T\sum_{e \in E}\lambda(e)ee^T)+o(h^2)\\
        & = h^2Tr(\nabla u(x)\nabla u(x)^T D)+o(h^2)\\
        & = h^2<\nabla u(x), D\nabla u(x)>+o(h^2)
    \end{split}
\end{equation}
\end{proof}



\section{Conclusion}
\section{Remerciements}
\section{Annexe}
\subsection{D√©monstration 1-lipsichianit√© du max}
\paragraph{Lemme:} Soit $(x_i)_{1\le i\le N},(y_j)_{1\le j\le N}\in \bR^N$, alors:
\begin{equation}
    |max_{1\le i\le N}(x_i)-max_{1\le j\le N}(y_j)|\le max_{1\le i\le N}(|x_i-y_i|)
\end{equation}
\paragraph{D√©monstration:}
Consid√©rons $(x_i)_{1\le i\le N},(y_j)_{1\le j\le N}\in \bR^N$.

Supposons $max_{1\le i\le N}(x_i) \ge max_{1\le j\le N}(y_j)$.

Posons $i_0\in [1,N]$ tel que $x_{i_0}=max_{1\le i\le N}(x_i)$.

On a alors:
\begin{equation}
\begin{split}
    |max_{1\le i\le N}(x_i)-max_{1\le j\le N}(y_j)| & = x_{i_0}-max_{1\le j\le N}(y_j) \\
    & \le x_{i_0}- y_{i_0}\\
    & \le |x_{i_0}- y_{i_0}|\\
    & \le max_{1\le i\le N}(|x_i-y_i|)
\end{split}
\end{equation}
Le cas $max_{1\le i\le N}(x_i) \le max_{1\le j\le N}(y_j)$ est sym√©trique.

\begin{proof}
\begin{lstlisting}[gobble=2]

    lemma max_1_lip (a b c d :‚Ñù ) :
        |(max 0 (max a b))-(max 0 (max c d))|‚â§ max (|a-c|) (|b-d|) :=
    begin
        let h1 := abs_max_sub_max_le_abs (max a b) (max c d) 0,
        let h2 := abs_max_sub_max_le_max a b c d,
        rw max_comm,
        rw max_comm 0 (max c d),
        exact le_trans h1 h2,
    end
\end{lstlisting}
\end{proof}

\section*{Exemple de code}

A code example:
\begin{lstlisting}[gobble=2]
  theorem selling_algorithm (vsb : is_direct_superbase v) (vint : ‚àÄ i j, v i j ‚àà Z)
    (Dsymm : D.is_symm) (Dposdef : D.pos_def) (hd : d = 2 ‚à® d = 3) :
    ‚àÉ v' : matrix (fin (d+1)) (fin d) ‚Ñù,
      is_direct_superbase v' ‚àß is_obtuse v' D ‚àß ‚àÄ i j, v' i j ‚àà Z :=
  sorry
\end{lstlisting}
We can also write the formula \lstinline{let f := Œª x, x ^ 2} inline.

$ x \vee y \wedge z \lambda {\color{symbolcolor}\ensuremath{\leq}} 3\uplambda$
${\color{symbolcolor}\ensuremath{\forall}}$
$Œµ$


\end{document}

